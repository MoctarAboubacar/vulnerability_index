# clear
rm(list = ls())

# load packages
require(tidyverse)
require(sf)
require(MASS)
require(car)
require(Hmisc)
require(psych)
require(corrplot)
require(factoextra)

# get and clean data ####
# load data
# df <- read.csv("C:/Users/moctar.aboubacar/Desktop/Vul Index/palikawise.csv")
shp <- st_read("C:/Users/moctar.aboubacar/Desktop/Vul Index/Nepal vulnerability shapefile/combined_data_sae_popn_building_vul_caste.shp")

# shape file to dataframe
df <- st_drop_geometry(shp)

# clean
## filter and create var
df <- df %>% 
  filter(LU_Type != "National Park",
         LU_Type != "Hunting Reserve",
         LU_Type != "Wildlife Reserve",
         Avg_P0WB > 0) %>% 
  mutate(bad_bldg = (bamboo + unbkbrck + woodplnk + mudbrkst) / (bamboo + unbkbrck + woodplnk + cembrkst + mudbrkst + notstat + others),
         caste = pcnvulcash / 100)

## subset
colname.fun <- function(df, cols){
  vec <- vector()
  for (i in cols){
    vec[i] <- which(colnames(df) == i)
  }
  return(vec)
}

cols <- colname.fun(df, c("HLCIT_CODE",
                          "Avg_P0",  "Avg_P0WB",  "Avg_S2", "Avg_W2", "Avg_U2",
                          "Avg_K0", "Avg_D_1",
                          "bad_bldg", "caste"))
bad_bldg <- df$bad_bldg
df <- df[cols]

# test ####
# scatterplotMatrix(df[2:10])
# rcorr(as.matrix(df[-1]))

# bad_bldg is a problem: the variable is very skewed, with median at  0.9
# otherwise there is no distribution/kewedness problem
# we try grouping it into 3 categories: 'passable', ''medium' and 'poor' levels of bad housing prevalence, based on lumping in the distribution

df <- df %>% 
  mutate(bldg_poor = ifelse(bad_bldg >= 0.91, 1, 0),
         bldg_med = ifelse(bad_bldg >= 0.7 & bad_bldg < 0.91, 1, 0),
         bldg_pass = ifelse(bad_bldg < 0.7, 1, 0))

dropcol <- colname.fun(df, c("bad_bldg", "bldg_med", "bldg_pass"))
df <- df[-dropcol]

table((df$bldg_pass)) / nrow(df)

# truncation: variables truncated and only at specific points on the 0-1 scale?

summary(df) # yes there is truncation because of the nature of prevalence of wasting (max 30%), and poverty (up to ~80%) 
# we need to standardize because the truncation at lower levels for the nutrition variables does not mean that we care less about them. 30% is very high wasting, whereas 30% food insecurity is not very high at all... for this reason we NEED to standardize our variables.

# reliability of the measure is very low (0.43) with all variables in, 0.6 when building is taken out and 0.8 when caste and building are taken out
alpha(df[-c(1, 9, 10)])
# as we want to ensure we are measuring the same thing, best to leave both of them out I think nd find a better way to integrate later
# the caste and building variables are problematic here, the theory being that they are not emprically measuring the same thing, the same idea underneath the variable. So we'll need a way to include them somehow (it'll have to just be cutoffs...) here's an idea study it in 3D space (1 the index 2 the caste % and the building %) or whatever and see how the cutoffs can work


# first PCA ####

# scale variables
df_scale <- as.data.frame(scale(df[2:8]))

# run algorithm
df.pca <- prcomp(df_scale)

summary(df.pca)
screeplot(df.pca, type = 'lines') # we see here that 2 components could be retained as they have eigenvalues above 1
df.pca$rotation[,1:2] # PC1 and PC 2 correspond exactly to food security on one hand and nutrition on the other. All eigenvectors are good (not too small) so we would retain all the variables

fviz_eig(df.pca)

fviz_pca_var(df.pca,
             col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)


df$PC1 <- df.pca$x[,1]
df$PC2 <- df.pca$x[,2]

# create quintiles and do internal coherence testing
df <- df %>% 
  mutate(PC1_quint = ntile(PC1, 5),
         PC2_quint = ntile(PC2, 5),
         PC2_dec = ntile(PC2, 10))

df.table1 <- df %>% 
  group_by(PC1_quint) %>% 
  summarise(Pov = mean(Avg_P0WB),
            FoodPov = mean(Avg_P0),
            Stunting = mean(Avg_S2),
            Wasting = mean (Avg_W2),
            Underweight = mean(Avg_U2),
            Kilocal = mean(Avg_K0),
            Diarrhoea = mean(Avg_D_1),
            Caste = mean(caste),
            bldg_poor = mean(bldg_poor))


df.table1 <- gather(df.table1, key = "key", value = "value", -PC1_quint)

ggplot(df.table1, aes(x = PC1_quint, y = value, group = key, color = key))+
  geom_line()+
  geom_point()
  
ggplot(df.table1, aes(x = PC1_quint, y = value))+
  geom_line()+
  geom_point()+
  facet_wrap(~ key, scales = 'free_y')


df.table2 <- df %>% 
  group_by(PC2_quint) %>% 
  summarise(Pov = mean(Avg_P0WB),
            FoodPov = mean(Avg_P0),
            Stunting = mean(Avg_S2),
            Wasting = mean (Avg_W2),
            Underweight = mean(Avg_U2),
            Kilocal = mean(Avg_K0),
            Diarrhoea = mean(Avg_D_1),
            Caste = mean(caste),
            bldg_poor = mean(bldg_poor))


df.table2 <- gather(df.table2, key = "key", value = "value", -PC2_quint)

ggplot(df.table2, aes(x = PC2_quint, y = value))+
  geom_line()+
  geom_point()+
  facet_wrap(~ key, scales = 'free_y')


# try internal coherence testing with deciles :: it works! changes are higher on the extremes--on more vulnerable deciles, so it makes sense to use deciles to differentiate groups. At least to differentiate the 10th and 9th deciles.
# 
# df.table3 <- df %>% 
#   group_by(PC1_dec) %>% 
#   summarise(Pov = mean(Avg_P0WB),
#             FoodPov = mean(Avg_P0),
#             Stunting = mean(Avg_S2),
#             Wasting = mean (Avg_W2),
#             Underweight = mean(Avg_U2),
#             Kilocal = mean(Avg_K0),
#             Diarrhoea = mean(Avg_D_1),
#             Caste = mean(caste),
#             bldg_poor = mean(bldg_poor))
# 
# 
# df.table3 <- gather(df.table3, key = "key", value = "value", -PC1_dec)
# 
# ggplot(df.table3, aes(x = PC1_dec, y = value, group = key, color = key))+
#   geom_line()+
#   geom_point()
# 
# ggplot(df.table3, aes(x = PC1_dec, y = value))+
#   geom_line()+
#   geom_point()+
#   facet_wrap(~ key, scales = 'free_y')
# 
# 
# df.table4 <- df %>% 
#   group_by(PC2_dec) %>% 
#   summarise(Pov = mean(Avg_P0WB),
#             FoodPov = mean(Avg_P0),
#             Stunting = mean(Avg_S2),
#             Wasting = mean (Avg_W2),
#             Underweight = mean(Avg_U2),
#             Kilocal = mean(Avg_K0),
#             Diarrhoea = mean(Avg_D_1),
#             Caste = mean(caste),
#             bldg_poor = mean(bldg_poor))
# 
# 
# df.table4 <- gather(df.table4, key = "key", value = "value", -PC2_dec)
# 
# ggplot(df.table4, aes(x = PC2_dec, y = value))+
#   geom_line()+
#   geom_point()+
#   facet_wrap(~ key, scales = 'free_y')

# map it out and see if it makes sense (quintiles and deciles)

shp <- merge(shp, df[c("HLCIT_CODE", "PC1", "PC1_quint", "PC2", "PC2_quint", "PC2_dec")], all.x = TRUE)
shp$PC1_quint[is.na(shp$PC1_quint)] <- 6
shp$PC2_quint[is.na(shp$PC2_quint)] <- 6
shp$PC2_dec[is.na(shp$PC2_dec)] <- 11

ggplot(shp)+
  geom_sf(aes(fill = factor(PC1_quint), size = NA))+
  theme_void()+
  scale_fill_manual(values = c("#fef0d9", "#fdcc8a", "#fc8d59", "#e34a33", "#b30000", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))


# ggplot(shp)+
#   geom_sf(aes(fill = factor(PC1_dec), size = NA))+
#   theme_void()+
#   scale_fill_manual(values = c("#fff7ec", "#fff7ec","#fee8c8","#fdd49e", "#fdbb84", "#fc8d59", "#ef6548", "#d7301f", "#b30000", "#7f0000", "grey30"),
#                     labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "NA"))


ggplot(shp)+
  geom_sf(aes(fill = factor(PC2_quint), size = NA))+
  theme_void()+
  scale_fill_manual(values = c("#fef0d9", "#fdcc8a", "#fc8d59", "#e34a33", "#b30000", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))


ggplot(shp)+
  geom_sf(aes(fill = factor(PC2_dec), size = NA))+
  theme_void()+
  scale_fill_manual(values = c("#fff7ec", "#fff7ec","#fee8c8","#fdd49e", "#fdbb84", "#fc8d59", "#ef6548", "#d7301f", "#b30000", "#7f0000", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "NA"))


## merging food security index with exposure (bad_bldg) and adaptive capacity (caste) information 



# all 4 vars (PC1 PC2, caste, bldg) need to be scaled between 0 and 1 first
range01 <- function(x, ...){(x-min(x, ...))/(max(x, ...)-min(x, ...))}
df <- df %>% 
  mutate(PC1_scale = range01(PC1),
         PC2_scale = range01(PC2),
         bad_bldg_scale = range01(bad_bldg),
         caste_scale = range01(caste),
         bad_bldg = bad_bldg,
         vul_1 = (PC1_scale * 0.7) + (bad_bldg_scale * 0.2) + (caste_scale * 0.1),
         vul_2 = (PC1_scale * 0.8) + (bad_bldg_scale * 0.1) + (caste_scale * 0.1),
         vul_3 = (PC1_scale * 0.6) + (bad_bldg_scale * 0.2) + (caste_scale * 0.2),
         vul_4 = (PC1_scale * 0.5) + (bad_bldg_scale * 0.25) + (caste_scale * 0.25),
         vul_corona = (PC1_scale * 0.75) + (caste_scale * 0.25),
         old_72h = factor(case_when(Avg_P0 > 0.2 ~ 1,
                                    Avg_P0 > 0.1 & Avg_P0 <= 0.2 ~ 2,
                                    Avg_P0 <= 0.1 ~ 3)))

glimpse(df)
shp <- merge(shp, df[c("HLCIT_CODE", "PC1", "PC1_scale", "PC2", "PC2_scale", "vul_3", "vul_corona", "old_72h", "vul_4", "vul_2")], all.x = TRUE)


shp <- shp %>% 
  mutate(PC1_quint = ntile(PC1, 5),
         PC2_quint = ntile(PC2, 5),
         vul_2_quint = ntile(vul_2, 5),
         vul_3_quint = ntile(vul_3, 5),
         vul_4_quint = ntile(vul_4, 5),
         vul_corona_quint = ntile(vul_corona, 5),# I'm liking this!! quintiles from the 60/20/20 weightin
         PC1_scale = range01(PC1, na.rm = T),
         PC2_scale = range01(PC2, na.rm = T)) 


############# evaluation and testing ####
# 1 compare the vulnerability measure vul_3 to the First Principal Component
conf.matrix1 <- table(shp$vul_3_quint, shp$PC1_quint)
conf.matrix2 <- table(shp$vul_corona_quint, shp$PC1_quint)
conf.matrix3 <- table(shp$vul_4_quint, shp$PC1_quint)

ggplot(shp, aes(x = vul_corona, y = PC1_scale, color = STATE ))+
  geom_point()+
  geom_smooth(se = F, method = 'lm')

require(caret)
confusionMatrix(factor(shp$vul_3_quint), factor(shp$PC1_quint))
confusionMatrix(factor(shp$vul_corona_quint), factor(shp$PC1_quint))
table(shp$vul_corona_quint, shp$PC1_quint)


ggplot(shp, aes(x = PC1))+
  geom_density(aes(fill = STATE))+
  facet_wrap(~STATE)

ggplot(shp, aes(x = vul_3))+
  geom_density(aes(fill = STATE))+
  facet_wrap(~STATE)

ggplot(shp, aes(x = vul_4))+
  geom_density(aes(fill = STATE))+
  facet_wrap(~STATE)


# export a CSV file of PC2 and vul_4
fs_vul <- as.data.frame(st_drop_geometry(shp[,colname.fun(shp, c("HLCIT_CODE", "STATE", "DISTRICT", "LU_Name", "PC2", "PC2_dec", "vul_4", "vul_4_quint"))]))


write.csv(fs_vul, file = "C:/Users/moctar.aboubacar/World Food Programme/SO5- Evidence Policy & Innovation - COVID-19 response/Targeting/fs_vul.csv")

ggplot(shp, aes(x = PC1_scale, y = vul_3, color = DISTRICT))+
  geom_point() # here we can see that the vulnerability index makes some changes (on about 30% of the palikas), but always within 1 class, and makes those changes largely equally (roughly equal number of palikas increase vs. decrease class compared to the PC1)


# 2 compare the vul_3  and vul_4 to the regular 72h categories
table(shp$vul_3_quint, shp$old_72h)
table(shp$vul_4_quint, shp$old_72h)
# compared to the old 72h index, there is a good agreement on the food secure areas from the old methodology, but some areas identified as most food insecure by the old methodology (1 in 7 or ~ 13%) are seen as least vulnerable under the new breakdown.
shp$vul_4_quint[is.na(shp$vul_4_quint)] <- 6
shp$PC2_quint[is.na(shp$PC2_quint)] <- 6



district <- shp %>% 
  group_by(DISTRICT) %>% 
  summarise(mean(PC1)) %>% 
  ms_simplify()

province <- shp %>% 
  group_by(STATE) %>% 
  summarise(mean(PC1)) %>% 
  ms_simplify()


old_72 <- ggplot(shp)+
  geom_sf(aes(fill = factor(old_72h)))+
  theme_void()+
  scale_fill_manual(values = c("#ef6548","#fdbb84","#fee8c8", "grey70"),
                    labels = c("1", "2", "3", "NA"))
vul_1 <- ggplot(shp)+
  geom_sf(aes(fill = factor(vul_1_quint)))+
  theme_void()+
  scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))
vul_2 <- ggplot(shp)+
  geom_sf(aes(fill = factor(vul_2_quint)))+
  theme_void()+
  scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))
vul_3 <- ggplot(shp)+
  geom_sf(data = district, aes(fill = NA), lwd = 0.7)+
  geom_sf(data = province, aes(fill = NA), lwd = 1.1)+
  geom_sf(aes(fill = factor(vul_3_quint)))+
  theme_void()+
  scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))





vul_4 <- ggplot(shp)+
  geom_sf(aes(fill = factor(vul_4_quint)))+
  geom_sf(data = district, aes(fill = NA), lwd = 0.7, color = 'black')+
  geom_sf(data = province, aes(fill = NA), lwd = 1.1, color = 'black')+
  theme_void()+
  scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
                    labels = c("1", "2", "3", "4", "5"))+
  labs(title = "72h Vulnerability Index",
       fill = "Quintiles")

PC2 <- ggplot(shp)+
  geom_sf(aes(fill = factor(PC2_quint)))+
  geom_sf(data = district, aes(fill = NA), lwd = 0.7, color = 'black')+
  geom_sf(data = province, aes(fill = NA), lwd = 1.1, color = 'black')+
  theme_void()+
  scale_fill_manual(values = c("#feebe2","#fbb4b9", "#f768a1", "#c51b8a", "#7a0177", "grey70"),
                    labels = c("1", "2", "3", "4", "5"))+
  labs(title = "72h Hygiene-Nutrition Severity Index",
       fill = "Quintiles")


compare <- gridExtra::grid.arrange(old_72, vul_2, vul_3, vul_4,
                        ncol = 2,
                        nrow = 2) # we see here that many Terai palikas that were Priority 1 before are now either 4/5 or 3/5 or sometimes 2/5 on the vulnerability scale. Overall this is ok though, since we are seeing these as priority scales, not as absolute measures. When there is a flood, we will only consider the range of those palikas concerned. We can think of some way to put those groups in the larger perspective of the country. We still need to think though: if after an EQ we put those categories out there...what are people supposed to do with that information? We could put it together with a stylization of the distribution, saying this is where those people fall when looking at the whole country... For a flood that might be irresponsible though...

# 3 compare to 2017 DFSN assessment (measures are different--food security vulnerability vs. how affected different areas are 1 month after the fact, my measure vs. the IPC-type measure). this is more for illustrative purposes...I guesS?

# load the data
DFSN <- read.csv("C:/Users/moctar.aboubacar/Desktop/Vul Index/IPC_2017_floods.csv")

# clean and merge with SHP dataset
DFSN <- DFSN %>% 
  group_by(LU_Name) %>% 
  summarise(Phase.2017 = round(mean(Phase.2017), 0))

shp <- merge(shp, DFSN[c("LU_Name", "Phase.2017")], all.x = TRUE)

shp_phase <- shp %>% 
  filter(DISTRICT == "BARA" | DISTRICT == "DHANUSHA" | DISTRICT == "JHAPA" | DISTRICT == "MAHOTTARI" | DISTRICT == "MORANG" | DISTRICT == "PARSA" | DISTRICT == "RAUTAHAT" | DISTRICT == "SAPTARI" | DISTRICT == "SARLAHI" | DISTRICT == "SIRAHA" | DISTRICT == "SUNSARI")


table(shp_phase$vul_3_quint, shp_phase$Phase.2017) # crucially, only 16 of those areas identified as 3 or 4 in the DFSN were identified as having vulnerability levels in the 1st or 2nd quintiles of the vulnerability index
table(shp_phase$vul_4_quint, shp_phase$Phase.2017)


  
phase2017 <- ggplot(shp_phase)+
    geom_sf(aes(fill = factor(Phase.2017)))+
    theme_void()+
    scale_fill_manual(values = c("#fee8c8","#fdbb84","#ef6548", "grey70"),
                      labels = c("1", "2", "3", "4"))

vul_3_terai <- ggplot(shp_phase)+
  geom_sf(aes(fill = factor(vul_3_quint)))+
  theme_void()+
  scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))

vul_2_terai <- ggplot(shp_phase)+
  geom_sf(aes(fill = factor(vul_2_quint)))+
  theme_void()+
  scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))

# vul_1_terai <- ggplot(shp_phase)+
#   geom_sf(aes(fill = factor(vul_1_quint)))+
#   theme_void()+
#   scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
#                     labels = c("1", "2", "3", "4", "5", "NA"))

vul_4_terai <- ggplot(shp_phase)+
  geom_sf(aes(fill = factor(vul_4_quint)))+
  theme_void()+
  scale_fill_manual(values = c("#fee8c8","#fdd49e", "#fdbb84", "#ef6548", "#d7301f", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))



gridExtra::grid.arrange(phase2017, vul_2_terai, vul_3_terai, vul_4_terai,
                                    ncol = 2,
                                    nrow = 2) # despite clear differences at a basic basic level it looks more or less consistent with these other measures. 




# import and use most affected groups

most.affected <- read_excel("C:/Users/moctar.aboubacar/Desktop/1_test_data.xlsx",
                 na = "#N/A")
most.affected <-  most.affected %>% 
  filter(LU_Type != "National Park",
         LU_Type != "Hunting Reserve",
         LU_Type != "Wildlife Reserve",
         S1.1.Avg_P0WB > 0) %>% 
  mutate(most_affected = S1.3.most_affected / Population2020,
         most_affected_quint = ntile(most_affected, 5))

most.affected <- most.affected[,colname.fun(most.affected, c("HLCIT_CODE", "GCODE", "DISTRICT", "STATE", "LU_Name", "Population2020", "most_affected", "most_affected_quint"))]


shp <- merge(shp, most.affected[c("HLCIT_CODE", "most_affected", "most_affected_quint")], all.x = TRUE)

shp$most_affected_quint[is.na(shp$most_affected_quint)] <- 6

map_most_affected <- ggplot(shp)+
  geom_sf(aes(fill = factor(most_affected_quint)))+
  geom_sf(data = district, aes(fill = NA), lwd = 0.7, color = 'black')+
  geom_sf(data = province, aes(fill = NA), lwd = 1.1, color = 'black')+
  theme_void()+
  scale_fill_manual(values = c("#f0f9e8","#bae4bc", "#7bccc4", "#43a2ca", "#0868ac", "grey70"),
                    labels = c("1", "2", "3", "4", "5", "NA"))+
  labs(title = "COVID-19 Percentage Economically Most Affected Households (quintiles)",
       fill = "Quintiles")







# we should be crystal clear that this is a relative scale, and we CANNOT make simple humanitarian programming based on an arbitrary cutoff. The FIELD ASSESSEMENTS are the critical things in saying what the situation is in the most vulnerable of the affected areas and in making determinations. This is really just that first look, first organization of Palikas.

# the internal coherence tests are...coherent given our identification of PC1 as poverty-driven food insecurity and PC2 as hygiene driven malnutrition
# it makes sense to keep these two to use together in the 72h analysis because: the second one is the malnutrition that is not driven by poverty or deprivation (or very little driven by that--check factor loading on Poverty Avg_P0WB). SO the main map will be PC1, and then PC2 can be applied within the flooded areas, according to the priority areas of intervention decided, to then say that the nutrition-type intervention (a separate intervention) should be focused here, here, here, etc). Remember per Cronbach's alpha, all of the variables are getting at the 'real construct' of food insecurity--the PC1/PC2 breakdown is just th edifferent components of FS they are addressing.

# POINTS TO ADDRESS/TEST
# how to include Caste? Take a look at the distribution in the Terai and see if it is 'underweighting' things compared to Karnali. Caste could be a way to 'rebalance' the main PC1 if it seems to be unfairly skewing away from the Terai places. Try scattering caste against PC1 and see if there is an ok trend to follow here....

# the note needs to be explicit that these are relative scores, meant to rank palikas but not say anything in the absolute. updated data when available + field reports etc. can give that info.

